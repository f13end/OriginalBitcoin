<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 3.2//EN">
<HTML>
 
<!-- Mirrored from listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/007282.html by HTTrack Website Copier/3.x [XR&CO'2010], Tue, 30 Aug 2011 14:46:34 GMT -->

<!-- Mirrored from diyhpl.us/~bryan/irc/bitcoin-satoshi/p2presearch-again/p2pfoundation.net/backups/p2p_research-archives/2010-February/007282.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 06 Dec 2018 08:57:32 GMT -->
<HEAD>
   <TITLE> [p2p-research] Applying utility functions to humans considered	harmful
   </TITLE>
   <LINK REL="Index" HREF="index.html" >
   <LINK REL="made" HREF="mailto:p2presearch%40listcultures.org?Subject=Re%3A%20%5Bp2p-research%5D%20Applying%20utility%20functions%20to%20humans%20considered%0A%09harmful&In-Reply-To=%3Cc776300b1002031820r423724f3g6e1060ff54ec8169%40mail.gmail.com%3E">
   <META NAME="robots" CONTENT="index,nofollow">
   <META http-equiv="Content-Type" content="text/html; charset=us-ascii">
   <LINK REL="Previous"  HREF="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/007281.html">
   <LINK REL="Next"  HREF="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/007395.html">
 </HEAD>
 <BODY BGCOLOR="#ffffff">
   <H1>[p2p-research] Applying utility functions to humans considered	harmful</H1>
    <B>Michel Bauwens</B> 
    <A HREF="mailto:p2presearch%40listcultures.org?Subject=Re%3A%20%5Bp2p-research%5D%20Applying%20utility%20functions%20to%20humans%20considered%0A%09harmful&In-Reply-To=%3Cc776300b1002031820r423724f3g6e1060ff54ec8169%40mail.gmail.com%3E"
       TITLE="[p2p-research] Applying utility functions to humans considered	harmful">michelsub2004 at gmail.com
       </A><BR>
    <I>Thu Feb  4 03:20:45 CET 2010</I>
    <P><UL>
        <LI>Previous message: <A HREF="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/007281.html">[p2p-research] Applying utility functions to humans considered	harmful
</A></li>
        <LI>Next message: <A HREF="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/007395.html">[p2p-research] Applying utility functions to humans considered	harmful
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/date.html#7282">[ date ]</a>
              <a href="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/thread.html#7282">[ thread ]</a>
              <a href="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/subject.html#7282">[ subject ]</a>
              <a href="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/author.html#7282">[ author ]</a>
         </LI>
       </UL>
    <HR>  
<!--beginarticle-->
<PRE>the pdf won't load here, but that seems to be the Van Gelder I was referring
to in a discussion with jandrews, as saying that equating brains and humans
as machines belonged to the infantile phase of AI,

Michel

On Thu, Feb 4, 2010 at 8:21 AM, Ryan &lt;<A HREF="http://listcultures.org/mailman/listinfo/p2presearch_listcultures.org">rlanham1963 at gmail.com</A>&gt; wrote:

&gt;<i> For the utilitarians out there...
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Sent to you by Ryan via Google Reader:
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Applying utility functions to humans considered harmful&lt;<A HREF="http://lesswrong.com/lw/1qk/applying_utility_functions_to_humans_considered/">http://lesswrong.com/lw/1qk/applying_utility_functions_to_humans_considered/</A>&gt;
</I>&gt;<i> via lesswrong: What's new &lt;<A HREF="http://lesswrong.com/">http://lesswrong.com/</A>&gt; on 2/3/10
</I>&gt;<i>
</I>&gt;<i> Submitted by Kaj_Sotala &lt;<A HREF="http://lesswrong.com/user/Kaj_Sotala">http://lesswrong.com/user/Kaj_Sotala</A>&gt; 19 comments&lt;<A HREF="http://lesswrong.com/lw/1qk/applying_utility_functions_to_humans_considered/#comments">http://lesswrong.com/lw/1qk/applying_utility_functions_to_humans_considered/#comments</A>&gt;
</I>&gt;<i>
</I>&gt;<i> There's a lot of discussion on this site that seems to be assuming
</I>&gt;<i> (implicitly or explicitly) that it's meaningful to talk about the utility
</I>&gt;<i> functions of individual humans. I would like to question this assumption.
</I>&gt;<i>
</I>&gt;<i> To clarify: I don't question that you couldn't, *in principle*, model* *a
</I>&gt;<i> human's preferences by building this insanely complex utility function. But
</I>&gt;<i> there's an infinite amount of methods by which you could model a human's
</I>&gt;<i> preferences. The question is which model is the most useful, and which
</I>&gt;<i> models have the least underlying assumptions that will lead your intuitions
</I>&gt;<i> astray.
</I>&gt;<i>
</I>&gt;<i> Utility functions are a good model to use if we're talking about designing
</I>&gt;<i> an AI. We want an AI to be predictable, to have stable preferences, and do
</I>&gt;<i> what we want. It is also a good tool for building agents that are immune to
</I>&gt;<i> Dutch book tricks. Utility functions are a bad model for beings that do not
</I>&gt;<i> resemble these criteria.
</I>&gt;<i>
</I>&gt;<i> To quote Van Gelder (1995)&lt;<A HREF="http://people.bu.edu/pbokulic/class/vanGelder-reading.pdf">http://people.bu.edu/pbokulic/class/vanGelder-reading.pdf</A>&gt;
</I>&gt;<i> :
</I>&gt;<i>
</I>&gt;<i> Much of the work within the classical framework is mathematically elegant
</I>&gt;<i> and provides a useful description of optimal reasoning strategies. As an
</I>&gt;<i> account of the actual decisions people reach, however, classical utility
</I>&gt;<i> theory is seriously flawed; human subjects typically deviate from its
</I>&gt;<i> recommendations in a variety of ways. As a result, many theories
</I>&gt;<i> incorporating variations on the classical core have been developed,
</I>&gt;<i> typically relaxing certain of its standard assumptions, with varying degrees
</I>&gt;<i> of success in matching actual human choice behavior.
</I>&gt;<i>
</I>&gt;<i> Nevertheless, virtually all such theories remain subject to some further
</I>&gt;<i> drawbacks:
</I>&gt;<i>
</I>&gt;<i> (1) They do not incorporate any account of the underlying motivations that
</I>&gt;<i> give rise to the utility that an object or outcome holds at a given time.
</I>&gt;<i> (2) They conceive of the utilities themselves as static values, and can
</I>&gt;<i> offer no good account of how and why they might change over time, and why
</I>&gt;<i> preferences are often inconsistent and inconstant.
</I>&gt;<i> (3) They offer no serious account of the deliberation process, with its
</I>&gt;<i> attendant vacillations, inconsistencies, and distress; and they have nothing
</I>&gt;<i> to say about the relationships that have been uncovered between time spent
</I>&gt;<i> deliberating and the choices eventually made.
</I>&gt;<i>
</I>&gt;<i> Curiously, these drawbacks appear to have a common theme; they all concern,
</I>&gt;<i> one way or another, *temporal* aspects of decision making. It is worth
</I>&gt;<i> asking whether they arise because of some deep structural feature inherent
</I>&gt;<i> in the whole framework which conceptualizes decision-making behavior in
</I>&gt;<i> terms of calculating expected utilities.
</I>&gt;<i>
</I>&gt;<i> One model that attempts to capture actual human decision making better is
</I>&gt;<i> called *decision field theory*. (I'm no expert on this theory, having
</I>&gt;<i> encountered it two days ago, so I can't vouch for how good it actually is.
</I>&gt;<i> Still, even if it's flawed, it's useful for getting us to think about human
</I>&gt;<i> preferences in what seems to be a more realistic way.) Here's a brief
</I>&gt;<i> summary of how it's constructed from traditional utility theory, based on Busemeyer
</I>&gt;<i> &amp; Townsend (1993) &lt;<A HREF="http://mypage.iu.edu/~jbusemey/psy_rev_1993.pdf">http://mypage.iu.edu/%7Ejbusemey/psy_rev_1993.pdf</A>&gt;. See
</I>&gt;<i> the article for the mathematical details, closer justifications and
</I>&gt;<i> different failures of classical rationality which the different stages
</I>&gt;<i> explain.
</I>&gt;<i>
</I>&gt;<i> *Stage 1: Deterministic Subjective Expected Utility (SEU) theory.*Basically classical utility theory. Suppose you can choose between two
</I>&gt;<i> different alternatives, A and B. If you choose A, there is a payoff of 200
</I>&gt;<i> utilons with probability S1, and a payoff of -200 utilons with probability
</I>&gt;<i> S2. If you choose B, the payoffs are -500 utilons with probability S1 and
</I>&gt;<i> +500 utilons with probability S2. You'll choose A if the expected utility of
</I>&gt;<i> A, S1 * 200 + S2 * -200 is higher than the expected utility of B, S1 * -500
</I>&gt;<i> + S2 * 500, and B otherwise.
</I>&gt;<i>
</I>&gt;<i> *Stage 2: Random SEU theory. *In stage 1, we assumed that the
</I>&gt;<i> probabilities S1 and S2 stay constant across many trials. Now, we assume
</I>&gt;<i> that sometimes the decision maker might focus on S1, producing a preference
</I>&gt;<i> for action A. On other trials, the decision maker might focus on S2,
</I>&gt;<i> producing a preference for action B. According to random SEU theory, the
</I>&gt;<i> attention weight for variable S*i* is a continous random variable, which
</I>&gt;<i> can change from trial to trial because of attentional fluctuations. Thus,
</I>&gt;<i> the SEU for each action is also a random variable, called the *valence* of
</I>&gt;<i> an action. Deterministic SEU is a special case of random SEU, one where the
</I>&gt;<i> trial-by-trial fluctuation of valence is zero.
</I>&gt;<i>
</I>&gt;<i> *Stage 3: Sequential SEU theory.* In stage 2, we assumed that one's
</I>&gt;<i> decision was based on just one sample of a valence difference on any trial.
</I>&gt;<i> Now, we allow a sequence of one or more samples to be accumulated during the
</I>&gt;<i> deliberation period of a trial. The attention of the decision maker shifts
</I>&gt;<i> between different anticipated payoffs, accumulating weight to the different
</I>&gt;<i> actions. Once the weight of one of the actions reaches some critical
</I>&gt;<i> threshold, that action is chosen. Random SEU theory is a special case of
</I>&gt;<i> sequential SEU theory, where the amount of trials is one.
</I>&gt;<i>
</I>&gt;<i> Consider a scenario where you're trying to make a very difficult, but very
</I>&gt;<i> important decisions. In that case, your inhibitory threshold for any of the
</I>&gt;<i> actions is very high, so you spend a lot of time considering the different
</I>&gt;<i> consequences of the decision before finally arriving to the (hopefully)
</I>&gt;<i> correct decision. For less important decisions, your inhibitory threshold is
</I>&gt;<i> much lower, so you pick one of the choices without giving it too much
</I>&gt;<i> thought.
</I>&gt;<i>
</I>&gt;<i> *Stage 4: Random Walk SEU theory. *In stage 3, we assumed that we begin to
</I>&gt;<i> consider each decision from a neutral point, without any of the actions
</I>&gt;<i> being the preferred one. Now, we allow prior knowledge or experiences to
</I>&gt;<i> bias the initial state. The decision maker may recall previous preference
</I>&gt;<i> states, that are influenced in the direction of the mean difference.
</I>&gt;<i> Sequential SEU theory is a special case of random walk theory, where the
</I>&gt;<i> initial bias is zero.
</I>&gt;<i>
</I>&gt;<i> Under this model, decisions favoring the status quo tend to be chosen more
</I>&gt;<i> frequently under a short time limit (low threshold), but a superior decision
</I>&gt;<i> is more likely to be chosen as the threshold grows. Also, if previous
</I>&gt;<i> outcomes have already biased decision A very strongly over B, then the mean
</I>&gt;<i> time to choose A will be short while the mean time to choose B will be long.
</I>&gt;<i>
</I>&gt;<i> *Stage 5: Linear System SEU theory. *In stage 4, we assumed that previous
</I>&gt;<i> experiences all contribute equally. Now, we allow the impact of a valence
</I>&gt;<i> difference to vary depending on whether it occurred early or late (a primacy
</I>&gt;<i> or recency effect &lt;<A HREF="http://en.wikipedia.org/wiki/Serial_position_effect">http://en.wikipedia.org/wiki/Serial_position_effect</A>&gt;).
</I>&gt;<i> Each previous experience is given a weight given by a growth-decay rate
</I>&gt;<i> parameter. Random walk SEU theory is a special case of linear system SEU
</I>&gt;<i> theory, where the growth-decay rate is set to zero.
</I>&gt;<i>
</I>&gt;<i> *Stage 6: Approach-Avoidance Theory. *In stage 5, we assumed that, for
</I>&gt;<i> example, the average amount of attention given to the payoff (+500) only
</I>&gt;<i> depended on event S2. Now, we allow the average weight to be affected by a
</I>&gt;<i> another variable, called the goal gradient. The basic idea is that the
</I>&gt;<i> attractiveness of a reward or the aversiveness of a punishment is a
</I>&gt;<i> decreasing function of distance from the point of commitment to an action.
</I>&gt;<i> If there is little or no possibility of taking an action, its consequences
</I>&gt;<i> are ignored; as the possibility of taking an action increases, the attention
</I>&gt;<i> to its consequences increases as well. Linear system theory is a special
</I>&gt;<i> case of approach-avoidance theory, where the goal gradient parameter is
</I>&gt;<i> zero.
</I>&gt;<i>
</I>&gt;<i> There are two different goal gradients, one for gains and rewards and one
</I>&gt;<i> for losses or punishments. Empirical research suggests that the gradient for
</I>&gt;<i> rewards tends to be flatter than that for punishments. One of the original
</I>&gt;<i> features of approach-avoidance theory was the distinction between rewards
</I>&gt;<i> versus punishments, closely corresponding to the distinction of positively
</I>&gt;<i> versus negatively framed outcomes made by more recent decision theorists.
</I>&gt;<i>
</I>&gt;<i> *Stage 7: Decision Field Theory. *In stage 6, we assumed that the time
</I>&gt;<i> taken to process each sampling is the same. Now, we allow this to change by
</I>&gt;<i> introducing into the theory a time unit *h*, representing the amount of
</I>&gt;<i> time it takes to retrieve and process one pair of anticipated consequences
</I>&gt;<i> before shifting attention to another pair of consequences. If *h* is
</I>&gt;<i> allowed to approach zero in the limit, the preference state evolves in an
</I>&gt;<i> approximately continous manner over time. Approach-avoidance is a spe... you
</I>&gt;<i> get the picture.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> ------------------------------
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Now, you could argue that all of the steps above are just artifacts of
</I>&gt;<i> being a bounded agent without enough computational resources to calculate
</I>&gt;<i> all the utilities precisely. And you'd be right. And maybe it's meaningful
</I>&gt;<i> to talk about the &quot;utility function of humanity&quot; as the outcome that occurs
</I>&gt;<i> when a CEV-like entity calculated what we'd decide if we could collapse
</I>&gt;<i> Decision Field Theory back into Deterministic SEU Theory. Or maybe you just
</I>&gt;<i> say that all of this is low-level mechanical stuff that gets included in the
</I>&gt;<i> &quot;probability of outcome&quot; computation of classical decision theory. But which
</I>&gt;<i> approach do you think gives us more useful conceptual tools in talking about
</I>&gt;<i> modern-day humans?
</I>&gt;<i>
</I>&gt;<i> You'll also note that even DFT (or at least the version of it summarized in
</I>&gt;<i> a 1993 article) assumes that the payoffs themselves do not change over time.
</I>&gt;<i> Attentional considerations might lead us to attach a low value to some
</I>&gt;<i> outcome, but if we were to actually end up in that outcome, we'd always
</I>&gt;<i> value it the same amount. This we know to be untrue. There's probably some
</I>&gt;<i> even better way of looking at human decision making, one which I suspect
</I>&gt;<i> might be very different from classical decision theory.
</I>&gt;<i>
</I>&gt;<i> So be extra careful when you try to apply the concept of a utility function
</I>&gt;<i> to human beings.
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> Things you can do from here:
</I>&gt;<i>
</I>&gt;<i>    - Subscribe to lesswrong: What's new&lt;<A HREF="http://www.google.com/reader/view/feed%2Fhttp%3A%2F%2Flesswrong.com%2F.rss?source=email">http://www.google.com/reader/view/feed%2Fhttp%3A%2F%2Flesswrong.com%2F.rss?source=email</A>&gt;using
</I>&gt;<i>    *Google Reader*
</I>&gt;<i>    - Get started using Google Reader&lt;<A HREF="http://www.google.com/reader/?source=email">http://www.google.com/reader/?source=email</A>&gt;to easily keep up with
</I>&gt;<i>    *all your favorite sites*
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i>
</I>&gt;<i> _______________________________________________
</I>&gt;<i> p2presearch mailing list
</I>&gt;<i> <A HREF="http://listcultures.org/mailman/listinfo/p2presearch_listcultures.org">p2presearch at listcultures.org</A>
</I>&gt;<i> <A HREF="http://listcultures.org/mailman/listinfo/p2presearch_listcultures.org">http://listcultures.org/mailman/listinfo/p2presearch_listcultures.org</A>
</I>&gt;<i>
</I>&gt;<i>
</I>

-- 
Work: <A HREF="http://en.wikipedia.org/wiki/Dhurakij_Pundit_University">http://en.wikipedia.org/wiki/Dhurakij_Pundit_University</A> - Think thank:
<A HREF="http://www.asianforesightinstitute.org/index.php/eng/The-AFI">http://www.asianforesightinstitute.org/index.php/eng/The-AFI</A>

P2P Foundation: <A HREF="http://p2pfoundation.net/">http://p2pfoundation.net</A>  - <A HREF="http://blog.p2pfoundation.net/">http://blog.p2pfoundation.net</A>

Connect: <A HREF="http://p2pfoundation.ning.com;/">http://p2pfoundation.ning.com;</A> Discuss:
<A HREF="http://listcultures.org/mailman/listinfo/p2presearch_listcultures.org">http://listcultures.org/mailman/listinfo/p2presearch_listcultures.org</A>

Updates: <A HREF="http://del.icio.us/mbauwens;">http://del.icio.us/mbauwens;</A> <A HREF="http://friendfeed.com/mbauwens;">http://friendfeed.com/mbauwens;</A>
<A HREF="http://twitter.com/mbauwens;">http://twitter.com/mbauwens;</A> <A HREF="http://www.facebook.com/mbauwens">http://www.facebook.com/mbauwens</A>
-------------- next part --------------
An HTML attachment was scrubbed...
URL: &lt;<A HREF="http://listcultures.org/pipermail/p2presearch_listcultures.org/attachments/20100204/6a24f0b0/attachment-0001.html">http://listcultures.org/pipermail/p2presearch_listcultures.org/attachments/20100204/6a24f0b0/attachment-0001.html</A>&gt;
</PRE>










































































































<!--endarticle-->
    <HR>
    <P><UL>
        <!--threads-->
	<LI>Previous message: <A HREF="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/007281.html">[p2p-research] Applying utility functions to humans considered	harmful
</A></li>
	<LI>Next message: <A HREF="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/007395.html">[p2p-research] Applying utility functions to humans considered	harmful
</A></li>
         <LI> <B>Messages sorted by:</B> 
              <a href="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/date.html#7282">[ date ]</a>
              <a href="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/thread.html#7282">[ thread ]</a>
              <a href="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/subject.html#7282">[ subject ]</a>
              <a href="http://listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/author.html#7282">[ author ]</a>
         </LI>
       </UL>

<hr>
<a href="http://listcultures.org/mailman/listinfo/p2presearch_listcultures.org">More information about the p2presearch
mailing list</a><br>
</body>
<!-- Mirrored from listcultures.org/pipermail/p2presearch_listcultures.org/2010-February/007282.html by HTTrack Website Copier/3.x [XR&CO'2010], Tue, 30 Aug 2011 14:46:34 GMT -->

<!-- Mirrored from diyhpl.us/~bryan/irc/bitcoin-satoshi/p2presearch-again/p2pfoundation.net/backups/p2p_research-archives/2010-February/007282.html by HTTrack Website Copier/3.x [XR&CO'2014], Thu, 06 Dec 2018 08:57:32 GMT -->
</html>
